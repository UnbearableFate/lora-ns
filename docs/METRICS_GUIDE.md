# MetaMathQA Metrics åŠŸèƒ½è¯´æ˜

## ğŸ“Š æ–°å¢åŠŸèƒ½æ¦‚è¿°

ä¸º MetaMathQA å’Œå…¶ä»–ç”Ÿæˆå¼ä»»åŠ¡æ·»åŠ äº†å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡æ”¯æŒã€‚

## ğŸ¯ æ–°å¢çš„ Metrics å‡½æ•°

### 1. `extract_math_answer(text: str) -> str`
**åŠŸèƒ½**: ä»æ•°å­¦é—®é¢˜çš„è§£ç­”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ

**æ”¯æŒçš„æ ¼å¼**:
- `"#### 42"` - MetaMathQA/GSM8K æ ‡å‡†æ ¼å¼
- `"The answer is 42"` - è‡ªç„¶è¯­è¨€æ ¼å¼
- `"Final answer: 42"` - æ˜ç¡®æ ‡æ³¨æ ¼å¼
- æå–æ–‡æœ¬ä¸­æœ€åä¸€ä¸ªæ•°å­—ï¼ˆå›é€€æ–¹æ¡ˆï¼‰

**ç¤ºä¾‹**:
```python
from utils.metrics import extract_math_answer

text1 = "We calculate: 2 + 2 = 4 #### 4"
answer1 = extract_math_answer(text1)  # Returns: "4"

text2 = "The answer is 42"
answer2 = extract_math_answer(text2)  # Returns: "42"

text3 = "Therefore x = -5.5"
answer3 = extract_math_answer(text3)  # Returns: "-5.5"
```

### 2. `normalize_answer(answer: str) -> str`
**åŠŸèƒ½**: æ ‡å‡†åŒ–ç­”æ¡ˆä»¥ä¾¿æ¯”è¾ƒ

**ç‰¹æ€§**:
- æ•°å€¼æ ‡å‡†åŒ–ï¼ˆå»é™¤å¤šä½™çš„é›¶ï¼‰
- åˆ†æ•°è½¬å°æ•°ï¼ˆå¦‚ "1/2" â†’ "0.5"ï¼‰
- å¤§å°å†™ä¸æ•æ„Ÿ

**ç¤ºä¾‹**:
```python
from utils.metrics import normalize_answer

normalize_answer("42")      # "42"
normalize_answer("42.0")    # "42"
normalize_answer("42.00")   # "42"
normalize_answer("3.140")   # "3.14"
normalize_answer("1/2")     # "0.5"
```

### 3. `compute_causal_lm_metrics(eval_preds) -> Dict[str, float]`
**åŠŸèƒ½**: è®¡ç®—åŸºç¡€å› æœè¯­è¨€æ¨¡å‹æŒ‡æ ‡

**è¿”å›æŒ‡æ ‡**:
- `token_accuracy`: Token çº§åˆ«çš„å‡†ç¡®ç‡

**é€‚ç”¨åœºæ™¯**: é€šç”¨çš„è¯­è¨€ç”Ÿæˆä»»åŠ¡

**ç¤ºä¾‹**:
```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒç”¨
# è¿”å›æ ¼å¼: {"token_accuracy": 0.85}
```

### 4. `compute_math_generation_metrics(tokenizer) -> Callable`
**åŠŸèƒ½**: åˆ›å»ºæ•°å­¦ç”Ÿæˆä»»åŠ¡çš„è¯„ä¼°å‡½æ•°

**è¿”å›æŒ‡æ ‡**:
- `token_accuracy`: Token çº§åˆ«çš„å‡†ç¡®ç‡
- `answer_accuracy`: æå–ç­”æ¡ˆçš„ç²¾ç¡®åŒ¹é…ç‡

**ç‰¹æ€§**:
- è‡ªåŠ¨è§£ç ç”Ÿæˆçš„æ–‡æœ¬
- æ™ºèƒ½æå–ç­”æ¡ˆ
- æ ‡å‡†åŒ–åæ¯”è¾ƒ
- è®°å½•æ ·ä¾‹ä¾›è°ƒè¯•

**é€‚ç”¨åœºæ™¯**: MetaMathQA, GSM8K ç­‰æ•°å­¦æ¨ç†ä»»åŠ¡

**ç¤ºä¾‹**:
```python
from transformers import AutoTokenizer
from utils.metrics import compute_math_generation_metrics

tokenizer = AutoTokenizer.from_pretrained("model_name")
compute_fn = compute_math_generation_metrics(tokenizer)

# åœ¨è®­ç»ƒæ—¶ä½¿ç”¨
# è¿”å›æ ¼å¼: {
#     "token_accuracy": 0.85,
#     "answer_accuracy": 0.72
# }
```

### 5. `get_metrics_function(task_name: str, tokenizer=None) -> Optional[Callable]`
**åŠŸèƒ½**: æ ¹æ®ä»»åŠ¡åç§°è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è¯„ä¼°å‡½æ•°

**æ”¯æŒçš„ä»»åŠ¡**:
- **GLUE ä»»åŠ¡**: `glue_sst2`, `glue_cola`, `glue_mrpc`, ç­‰
- **æ•°å­¦ä»»åŠ¡**: `metamath_qa`, `gsm8k`, åŒ…å« "math" çš„ä»»åŠ¡
- **ä»£ç ä»»åŠ¡**: åŒ…å« "code" çš„ä»»åŠ¡
- **é€šç”¨ LM**: åŒ…å« "causal" æˆ– "lm" çš„ä»»åŠ¡

**ç¤ºä¾‹**:
```python
from utils.metrics import get_metrics_function

# æ•°å­¦ä»»åŠ¡
metrics_fn = get_metrics_function("metamath_qa", tokenizer=tokenizer)

# GLUE ä»»åŠ¡
metrics_fn = get_metrics_function("glue_sst2")

# æœªçŸ¥ä»»åŠ¡
metrics_fn = get_metrics_function("unknown_task")  # Returns: None
```

## ğŸ”§ åœ¨ Trainer ä¸­çš„é›†æˆ

### è‡ªåŠ¨é›†æˆ
åœ¨ `trainer/trainer_preparation.py` çš„ `train_causal_lm_task` å‡½æ•°ä¸­ï¼Œmetrics ä¼šè‡ªåŠ¨æ ¹æ®ä»»åŠ¡åç§°é€‰æ‹©ï¼š

```python
# è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹å¹¶é€‰æ‹© metrics
task_name = config.get("task_name", "")
compute_metrics = get_metrics_function(task_name, tokenizer=tokenizer)

if compute_metrics:
    logger.info(f"Using metrics for task: {task_name}")
    # æ·»åŠ åˆ° trainer
    common_trainer_params["compute_metrics"] = compute_metrics
else:
    logger.info(f"No metrics defined, using loss only")
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

å¯¹äº MetaMathQA (`configs/smol/135m_metamath.yaml`):
```yaml
task_name: "metamath_qa"  # è‡ªåŠ¨ä½¿ç”¨ compute_math_generation_metrics
task_type: "CAUSAL_LM"

training:
  metric_for_best_model: "answer_accuracy"  # å¯ä»¥é€‰æ‹©ä¿å­˜æœ€ä½³ç­”æ¡ˆå‡†ç¡®ç‡çš„æ¨¡å‹
  greater_is_better: true
```

## ğŸ“ˆ è®­ç»ƒæ—¶çš„è¾“å‡º

### è¯„ä¼°æ—¥å¿—ç¤ºä¾‹
```
Evaluation metrics:
  - eval_loss: 1.234
  - eval_token_accuracy: 0.8567
  - eval_answer_accuracy: 0.7234
  - eval_runtime: 12.34s
  - eval_samples_per_second: 162.3
```

### è°ƒè¯•ä¿¡æ¯
åœ¨è¯„ä¼°æ—¶ä¼šè®°å½•å‰3ä¸ªæ ·ä¾‹ä¾›è°ƒè¯•ï¼š
```
============================================================
Sample predictions (for debugging):

Example 1:
  Prediction: Below is an instruction...#### 42...
  Label: Below is an instruction...#### 42...
  Extracted pred answer: 42
  Extracted label answer: 42
  Match: True

Example 2:
  Prediction: ...
  ...
============================================================
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. MetaMathQA è®­ç»ƒ
```bash
python train.py --config configs/smol/135m_metamath.yaml
```

è‡ªåŠ¨è·å¾—ï¼š
- âœ… Token å‡†ç¡®ç‡ - è¡¡é‡æ•´ä½“ç”Ÿæˆè´¨é‡
- âœ… Answer å‡†ç¡®ç‡ - è¡¡é‡æ•°å­¦æ¨ç†èƒ½åŠ›

### 2. GSM8K è®­ç»ƒ
```yaml
task_name: "gsm8k"  # åŒæ ·ä½¿ç”¨æ•°å­¦ metrics
```

### 3. é€šç”¨ Causal LM
```yaml
task_name: "causal_lm"  # ä½¿ç”¨åŸºç¡€ token accuracy
```

### 4. åªä½¿ç”¨ Lossï¼ˆä¸éœ€è¦ metricsï¼‰
```yaml
task_name: "some_custom_task"  # å¦‚æœä¸åŒ¹é…ä»»ä½•è§„åˆ™ï¼Œåªä½¿ç”¨ loss
```

## ğŸ“Š Metrics æ¯”è¾ƒ

| Metric | è®¡ç®—æ–¹å¼ | é€‚ç”¨ä»»åŠ¡ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|--------|---------|---------|------|------|
| **Loss** | äº¤å‰ç†µ | æ‰€æœ‰ä»»åŠ¡ | ç›´æ¥åæ˜ è®­ç»ƒç›®æ ‡ | ä¸å¤Ÿç›´è§‚ |
| **Token Accuracy** | Token çº§åˆ«åŒ¹é… | ç”Ÿæˆä»»åŠ¡ | ç»†ç²’åº¦è¯„ä¼° | å¯¹æ•´ä½“è´¨é‡ä¸æ•æ„Ÿ |
| **Answer Accuracy** | æå–ç­”æ¡ˆååŒ¹é… | æ•°å­¦/QA | ç›´æ¥è¯„ä¼°ä»»åŠ¡ç›®æ ‡ | éœ€è¦ç­”æ¡ˆæå–é€»è¾‘ |

## ğŸ” é«˜çº§é…ç½®

### 1. è‡ªå®šä¹‰ metric_for_best_model

åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šç”¨å“ªä¸ªæŒ‡æ ‡ä¿å­˜æœ€ä½³æ¨¡å‹ï¼š

```yaml
training:
  load_best_model_at_end: true
  metric_for_best_model: "answer_accuracy"  # æˆ– "token_accuracy" æˆ– "loss"
  greater_is_better: true  # answer_accuracy è¶Šå¤§è¶Šå¥½
```

### 2. è°ƒæ•´è¯„ä¼°é¢‘ç‡

```yaml
training:
  total_eval_times: 20  # æ€»å…±è¯„ä¼°20æ¬¡
  # æˆ–ç›´æ¥è®¾ç½®
  eval_steps: 100       # æ¯100æ­¥è¯„ä¼°ä¸€æ¬¡
```

### 3. ç¦ç”¨æŸäº› metrics

å¦‚æœä¸æƒ³ä½¿ç”¨è‡ªåŠ¨ metricsï¼Œå¯ä»¥ï¼š
- ä½¿ç”¨ä¸åŒ¹é…çš„ task_name
- æˆ–ä¿®æ”¹ä»£ç åœ¨ `get_metrics_function` ä¸­è¿”å› None

## ğŸ“ æ³¨æ„äº‹é¡¹

### 1. å†…å­˜ä½¿ç”¨
- ä½¿ç”¨ `preprocess_logits_for_metrics` å‡å°‘å†…å­˜
- åªä¿å­˜ argmax é¢„æµ‹ï¼Œä¸ä¿å­˜å®Œæ•´ logits

### 2. ç­”æ¡ˆæå–å‡†ç¡®æ€§
- ä¾èµ–äºæ–‡æœ¬æ ¼å¼
- å»ºè®®åœ¨è®­ç»ƒæ•°æ®ä¸­ä½¿ç”¨ç»Ÿä¸€çš„ç­”æ¡ˆæ ¼å¼ï¼ˆå¦‚ "#### answer"ï¼‰
- å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ `extract_math_answer` å‡½æ•°

### 3. è¯„ä¼°é€Ÿåº¦
- Answer accuracy éœ€è¦è§£ç ï¼Œä¼šç¨æ…¢
- å¯ä»¥é€šè¿‡å‡å°‘ `eval_batch_size` æ¥èŠ‚çœå†…å­˜
- åœ¨å¤§è§„æ¨¡è¯„ä¼°æ—¶è€ƒè™‘ä½¿ç”¨å­é›†

## ğŸ› ï¸ æ‰©å±•æ–¹æ³•

å¦‚æœéœ€è¦æ·»åŠ æ–°çš„ metricsï¼š

1. åœ¨ `utils/metrics.py` ä¸­åˆ›å»ºæ–°å‡½æ•°ï¼š
```python
def compute_my_custom_metrics(tokenizer):
    def compute_metrics(eval_preds):
        # ä½ çš„é€»è¾‘
        return {"my_metric": value}
    return compute_metrics
```

2. åœ¨ `get_metrics_function` ä¸­æ·»åŠ æ¡ä»¶ï¼š
```python
elif "my_task" in task_name.lower():
    return compute_my_custom_metrics(tokenizer)
```

## âœ… æ€»ç»“

ç°åœ¨é¡¹ç›®å®Œå…¨æ”¯æŒ MetaMathQA çš„è¯„ä¼°ï¼

**å…³é”®æ”¹è¿›**:
- âœ… æ™ºèƒ½ç­”æ¡ˆæå–
- âœ… æ ‡å‡†åŒ–æ¯”è¾ƒ
- âœ… Token å’Œ Answer åŒé‡å‡†ç¡®ç‡
- âœ… è‡ªåŠ¨ä»»åŠ¡æ£€æµ‹
- âœ… è°ƒè¯•å‹å¥½çš„æ—¥å¿—
- âœ… å†…å­˜ä¼˜åŒ–

**å¼€å§‹ä½¿ç”¨**:
```bash
python train.py --config configs/smol/135m_metamath.yaml
```

è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨è®¡ç®—å’Œè®°å½• `token_accuracy` å’Œ `answer_accuracy`ï¼
