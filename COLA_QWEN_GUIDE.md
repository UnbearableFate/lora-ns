# GLUE CoLA with Qwen2.5-1.5B - ä½¿ç”¨è¯´æ˜

## ä»»åŠ¡æè¿°

**CoLA (Corpus of Linguistic Acceptability)** æ˜¯GLUEåŸºå‡†æµ‹è¯•ä¸­çš„ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ï¼Œç”¨äºåˆ¤æ–­è‹±è¯­å¥å­åœ¨è¯­è¨€å­¦ä¸Šæ˜¯å¦å¯æ¥å—ã€‚

- **ä»»åŠ¡ç±»å‹**: äºŒåˆ†ç±»ï¼ˆå¯æ¥å—/ä¸å¯æ¥å—ï¼‰
- **è®­ç»ƒæ ·æœ¬**: 8,551æ¡
- **éªŒè¯æ ·æœ¬**: 1,043æ¡
- **è¯„ä¼°æŒ‡æ ‡**: Matthews Correlation Coefficient (MCC)

## æ¨¡å‹é…ç½®

### åŸºç¡€æ¨¡å‹
- **æ¨¡å‹**: Qwen/Qwen2.5-1.5B
- **å‚æ•°é‡**: 1.5B
- **æ¶æ„**: Qwenç³»åˆ—ï¼Œç±»ä¼¼äºLlamaæ¶æ„

### PEFTé…ç½®
- **æ–¹æ³•**: LoRA
- **Rank (r)**: 8
- **Alpha**: 16
- **Dropout**: 0.1
- **ç›®æ ‡æ¨¡å—**: q_proj, k_proj, v_proj, o_proj
- **å¯è®­ç»ƒå‚æ•°**: ~1M (ä»…0.067%çš„åŸå§‹å‚æ•°)

### è®­ç»ƒå‚æ•°
- **è®­ç»ƒè½®æ•°**: 10 epochs
- **æ‰¹æ¬¡å¤§å°**: 16 (æ¯è®¾å¤‡)
- **æ¢¯åº¦ç´¯ç§¯**: 2æ­¥
- **æœ‰æ•ˆæ‰¹æ¬¡**: 32
- **å­¦ä¹ ç‡**: 3e-4
- **ä¼˜åŒ–å™¨**: AdamW
- **æ··åˆç²¾åº¦**: BF16

## å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒæ¨¡å‹
```bash
# ä½¿ç”¨è®­ç»ƒè„šæœ¬
bash examples/train_cola_qwen.sh

# æˆ–ç›´æ¥è¿è¡Œ
python train.py --config configs/glue_cola_qwen.yaml
```

### 2. ç›‘æ§è®­ç»ƒ
```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir ./outputs/glue_cola_qwen/logs
```

### 3. æ¨ç†æµ‹è¯•
```bash
python inference.py \
  --config configs/glue_cola_qwen.yaml \
  --model_path ./outputs/glue_cola_qwen \
  --input_text "The book was written by John."
```

## ç¤ºä¾‹å¥å­

### å¯æ¥å—çš„å¥å­ (Label=1)
```
"The book was written by John."
"She gave him a present."
"I think that he is smart."
```

### ä¸å¯æ¥å—çš„å¥å­ (Label=0)
```
"The book was written John."  # ç¼ºå°‘ä»‹è¯
"She gave he a present."       # é”™è¯¯çš„ä»£è¯æ ¼
"I think that is he smart."    # é”™è¯¯çš„è¯­åº
```

## é¢„æœŸç»“æœ

åœ¨CoLAéªŒè¯é›†ä¸Šçš„é¢„æœŸæ€§èƒ½ï¼š
- **Matthews Correlation**: ~0.50-0.60
- **Accuracy**: ~80-85%

è®­ç»ƒæ—¶é—´ï¼ˆå•GPU A100ï¼‰ï¼š
- **é¢„è®¡æ—¶é—´**: 15-20åˆ†é’Ÿ
- **GPUå†…å­˜**: ~8-10GB

## é…ç½®æ–‡ä»¶è¯´æ˜

é…ç½®æ–‡ä»¶ä½ç½®: `configs/glue_cola_qwen.yaml`

å…³é”®é…ç½®é¡¹ï¼š
```yaml
# æ•°æ®é›†ç‰¹å®šé…ç½®
dataset:
  subset: "cola"              # CoLAå­ä»»åŠ¡
  text_column: ["sentence"]   # å•å¥è¾“å…¥
  num_labels: 2               # äºŒåˆ†ç±»

# è¯„ä¼°é…ç½®
training:
  metric_for_best_model: "matthews_correlation"  # CoLAä½¿ç”¨MCCä½œä¸ºä¸»è¦æŒ‡æ ‡
  greater_is_better: true
```

## å¾®è°ƒå»ºè®®

### æé«˜æ€§èƒ½
1. **å¢åŠ LoRA rank**: å°†`lora_r`ä»8å¢åŠ åˆ°16
2. **å¢åŠ è®­ç»ƒè½®æ•°**: ä»10å¢åŠ åˆ°15-20
3. **è°ƒæ•´å­¦ä¹ ç‡**: å°è¯•2e-4æˆ–5e-4
4. **ç›®æ ‡æ›´å¤šå±‚**: æ·»åŠ `gate_proj`, `up_proj`, `down_proj`

### å†…å­˜ä¼˜åŒ–
å¦‚æœé‡åˆ°OOMé—®é¢˜ï¼š
```yaml
training:
  per_device_train_batch_size: 8   # å‡å°æ‰¹æ¬¡
  gradient_accumulation_steps: 4    # å¢åŠ ç´¯ç§¯
  gradient_checkpointing: true      # å¯ç”¨æ£€æŸ¥ç‚¹
```

### ä½¿ç”¨é‡åŒ–
å¯¹äºä½æ˜¾å­˜GPUï¼ˆ<8GBï¼‰ï¼š
```yaml
training:
  optim: "paged_adamw_8bit"  # 8-bitä¼˜åŒ–å™¨
```

## è¯„ä¼°æŒ‡æ ‡

CoLAä»»åŠ¡ä½¿ç”¨Matthewsç›¸å…³ç³»æ•°(MCC)ä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡ã€‚

æˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶ä¼šè®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š
- **matthews_correlation**: Matthewsç›¸å…³ç³»æ•°ï¼ˆCoLAä¸»è¦æŒ‡æ ‡ï¼‰
- **accuracy**: å‡†ç¡®ç‡
- **f1**: F1åˆ†æ•°
- **precision**: ç²¾ç¡®ç‡
- **recall**: å¬å›ç‡

MCCçš„ä¼˜ç‚¹ï¼š
- å¹³è¡¡è€ƒè™‘æ‰€æœ‰æ··æ·†çŸ©é˜µå…ƒç´ 
- é€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®
- å–å€¼èŒƒå›´: -1åˆ°+1ï¼ˆ0è¡¨ç¤ºéšæœºé¢„æµ‹ï¼‰

## å¤šGPUè®­ç»ƒ

ä½¿ç”¨Accelerateè¿›è¡Œå¤šGPUè®­ç»ƒï¼š

```bash
# é…ç½®Accelerateï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
accelerate config

# å¯åŠ¨å¤šGPUè®­ç»ƒ
accelerate launch train.py --config configs/glue_cola_qwen.yaml
```

## æ•…éšœæ’é™¤

### 1. æ¨¡å‹ä¸‹è½½é—®é¢˜
å¦‚æœæ— æ³•ä¸‹è½½Qwenæ¨¡å‹ï¼š
```bash
# è®¾ç½®é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½åä½¿ç”¨æœ¬åœ°è·¯å¾„
# ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ model.name_or_path
```

### 2. å†…å­˜ä¸è¶³
- å‡å°æ‰¹æ¬¡å¤§å°: `per_device_train_batch_size: 8`
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: `gradient_checkpointing: true`
- ä½¿ç”¨8-bité‡åŒ–: `optim: "paged_adamw_8bit"`

### 3. è®­ç»ƒä¸æ”¶æ•›
- é™ä½å­¦ä¹ ç‡: `learning_rate: 2e-4`
- å¢åŠ warmup: `warmup_ratio: 0.2`
- æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®

## å‚è€ƒèµ„æ–™

- **CoLAè®ºæ–‡**: https://nyu-mll.github.io/CoLA/
- **GLUEåŸºå‡†**: https://gluebenchmark.com/
- **Qwenæ¨¡å‹**: https://huggingface.co/Qwen
- **LoRAè®ºæ–‡**: https://arxiv.org/abs/2106.09685

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€
