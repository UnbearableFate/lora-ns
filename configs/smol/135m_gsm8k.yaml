# GSM8K Task Configuration for SmolLM2-135M
task_name: "gsm8k"
task_type: "CAUSAL_LM"

# Model Configuration
model:
  name_or_path: "HuggingFaceTB/SmolLM2-135M"
  trust_remote_code: true

# PEFT Configuration
peft:
  method: "lora"
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  init_lora_weights: gaussian  # default: gaussian; options: pissa, lora_ga, lora_ns
  use_dora: false
  use_qalora: false
  use_rslora: false

# LoRA-GA Configuration (if using lora_ga or lora_ns)
loraga:
  batch_size: 32
  num_samples: 1024
  gradient:
    quant_flag: false
    save_path: data_cache/gradients

# Trainer Configuration
trainer:
  name: "Trainer"  # Options: "SpectralRefactorTrainer", "Trainer"
  refactor_every: 100
  warmup_steps: 0
  refactor_mode: "balanced"  # Options: "balanced", "exact"
  balance_lambda: 0.8
  preserve_momentum: true
  clear_momentum: false
  damping_eps: 1e-5
  clip_min_sigma: 0.0

# Dataset Configuration
dataset:
  name: "gsm8k"
  subset: "main"  # GSM8K has 'main' subset
  train_split: "train"  # GSM8K has ~7.5k training examples
  eval_split: "test"  # GSM8K has ~1.3k test examples
  test_split: null
  text_column: null  # Will be formatted in code
  label_column: null
  max_length: 512  # Reduced from 2048 for smaller model
  preprocessing_num_workers: 4
  
  # Prompt template for GSM8K math problems
  # GSM8K uses 'question' and 'answer' fields
  prompt_template: |
    Below is a math problem. Solve it step by step and provide the final answer.
    
    ### Question:
    {question}
    
    ### Answer:
    {answer}

# Training Arguments
training:
  output_dir: "./outputs/smol_135m_gsm8k/Trainer"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2  # Effective batch size = 4 * 2 = 8
  learning_rate: 0.0002  # 3e-4, slightly higher for smaller model
  weight_decay: 0.01
  warmup_ratio: 0.06
  lr_scheduler_type: "cosine"
  
  # Evaluation
  total_eval_times: 30
  load_best_model_at_end: true
  metric_for_best_model: "answer_accuracy"  # Use answer accuracy for math tasks
  greater_is_better: true  # Higher answer accuracy is better
  save_total_limit: 3
  
  # Optimization
  fp16: false
  bf16: true  # Use bf16 for better numerical stability
  gradient_checkpointing: false  # Not needed for small model
  optim: "adamw_torch_fused"
  max_grad_norm: 1.0
  
  # Logging
  logging_steps: 50
  logging_dir: "./outputs/smol_135m_gsm8k/logs"
  report_to: ["wandb"]
  
  # Misc
  seed: 42
  data_seed: 42
  remove_unused_columns: false
  
  # DataLoader settings
  dataloader_pin_memory: true
  dataloader_num_workers: 2
  dataloader_persistent_workers: false
  dataloader_prefetch_factor: 2

# WandB Configuration
wandb:
  project: "SmolLM2-135M-GSM8K"
  run_name_suffix: "spectral_refactor_pissa"
  online: false
