# MetaMathQA Task Configuration for Qwen3-1.7B
# LoRA fine-tuning on MetaMathQA math reasoning dataset

# Task Configuration
task_name: "metamath_qa"
task_type: "CAUSAL_LM"

# Model Configuration
model:
  name_or_path: "Qwen/Qwen3-1.7B"
  trust_remote_code: true
  token: true  # Requires HuggingFace token for gated model

# PEFT Configuration
peft:
  method: "lora"
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"
  init_lora_weights: true
  use_dora: false
  use_qalora: false
  use_rslora: false

# LoRA-GA Configuration (if using lora_ga or lora_ns)
loraga:
  batch_size: 64
  num_samples: 2048
  gradient:
    quant_flag: false
    save_path: data_cache/gradients

# Trainer Configuration
trainer:
  name: "Trainer"
  refactor_every: 200
  warmup_steps: 100
  refactor_mode: "balanced"
  balance_lambda: 0.85
  preserve_momentum: true
  clear_momentum: false
  damping_eps: 1e-6
  clip_min_sigma: 0.0

# Dataset Configuration
dataset:
  name: "meta-math/MetaMathQA"
  subset: null
  train_split: "train"
  eval_split: "train[:5000]"
  test_split: null
  max_length: 2048  # allow longer reasoning chains; still truncated if exceeded
  preprocessing_num_workers: 16
  input_fields: ["query"]
  target_field: "response"
  prompt_template: |
    Below is an instruction that describes a math problem. Provide a complete, correct solution.
    
    ### Instruction:
    {query}
    
    ### Response:
  response_template: |
    {target}
  append_eos_token: true
  add_bos_token: false
  truncate_prompt_only: true
  completion_prefix: ""
  completion_suffix: ""
  strip_prompt: true
  apply_chat_template: false

# Training Arguments
training:
  output_dir: "./outputs/qwen3_1.7b_metamath/Trainer"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  
  # Evaluation
  total_eval_times: 40
  load_best_model_at_end: true
  metric_for_best_model: "answer_accuracy"
  greater_is_better: true
  save_total_limit: 3
  
  # Optimization
  fp16: false
  bf16: true
  gradient_checkpointing: false
  optim: "adamw_torch"
  max_grad_norm: 1.0
  
  # Logging
  logging_steps: 50
  logging_dir: "./outputs/qwen3_1.7b_metamath/logs"
  report_to: ["wandb"]
  
  # Distributed Training
  ddp_find_unused_parameters: false
  ddp_backend: "nccl"
  
  # Misc
  seed: 42
  data_seed: 42
  remove_unused_columns: false
  
  # DataLoader settings
  dataloader_pin_memory: true
  dataloader_num_workers: 8
  dataloader_persistent_workers: true
  dataloader_prefetch_factor: 2

# WandB Configuration
wandb:
  project: "Qwen3-1.7B-MetaMath"
  online: true
  tags: ["qwen3-1.7b", "lora", "metamathqa"]
