# GLUE STS-B Task Configuration for RoBERTa-base
# LoRA fine-tuning for sentence similarity regression

# Model Configuration
model:
  name_or_path: "FacebookAI/roberta-large"
  trust_remote_code: true
  token: true
  attn_implementation: "sdpa"
  dtype: "bfloat16"

# PEFT Configuration
peft:
  variant: "lora"
  lora_r: 16
  lora_alpha: 1
  lora_dropout: 0.0
  target_modules: ["query", "key", "value", "dense"]
  exclude_modules: ["pooler.dense", "classifier.dense"]
  bias: "none"
  task_type: "SEQ_CLS"
  init_lora_weights: true # ("eva" "corda" "lora_ga" "gaussian" "true" "olora" "pissa" "orthogonal" )
  lora_init_kwargs:
    init_num_samples: 2048
    init_batch_size: 32
    corda_method: kpm
    init_seed: 12345

# Dataset Configuration
dataset:
  name: "glue"
  subset: "stsb"
  train_split: "train"
  eval_split: "validation"
  test_split: "test"
  text_column: ["sentence1", "sentence2"]
  label_column: "label"
  num_labels: 1
  max_length: 512
  preprocessing_num_workers: 16

trainer:
  name: "Trainer"   # or "CleanedSvdRefactorTrainer"
  min_alpha_ratio: 0.8
  max_alpha_ratio: 1.25
  repeat_n: 1
  final_decay_type: "linear"  # "linear" or "cosine"
  repeat_decay_type: "linear"  # "linear" or "cosine"
  repeat_warmup_ratio: 0.06
  repeat_decay_ratio: 0.06
  repeat_end_lr_rate: 0.94
  final_warmup_ratio: 0.06
  min_lr_rate: 0.0001
  warmup_start_lr_rate: 0.1
  first_warmup_start_lr_rate: 0.0001
  last_epoch: -1

# Training Arguments
training:
  output_dir: "./outputs"
  num_train_epochs: 10
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 256
  global_batch_size: 8
  learning_rate: 4e-4
  weight_decay: 0.0
  warmup_ratio: 0.06
  lr_scheduler_type: "linear"

  # Evaluation
  total_eval_times: 50
  load_best_model_at_end: true
  metric_for_best_model: "pearson"
  greater_is_better: true
  save_total_limit: 2
  run_validation_eval: true

  # Optimization
  fp16: false
  bf16: true
  gradient_checkpointing: false
  optim: "adamw_torch"
  max_grad_norm: 1.0

  # Logging
  logging_steps: 50
  report_to: ["wandb"]

  remove_unused_columns: true

  # DataLoader settings
  dataloader_pin_memory: true
  dataloader_num_workers: 16
  dataloader_persistent_workers: true
  dataloader_prefetch_factor: 2

# Generation (unused for classification, kept for config parity)
generation:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  batch_size: 8
  output_dir: "./results_r16"

# WandB Configuration
wandb:
  enabled: false
  project: "test0"
  online: true
  tags: ["stsb", "roberta-base", "lora"]