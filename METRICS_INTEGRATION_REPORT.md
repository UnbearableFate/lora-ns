# MetaMathQA Metrics é›†æˆå®ŒæˆæŠ¥å‘Š

## âœ… å®Œæˆçš„å·¥ä½œ

### 1. **æ‰©å±•äº† `utils/metrics.py`**

æ–°å¢äº†ä»¥ä¸‹å‡½æ•°ï¼š

#### æ ¸å¿ƒå‡½æ•°
- âœ… `extract_math_answer(text)` - ä»è§£ç­”ä¸­æå–ç­”æ¡ˆ
- âœ… `normalize_answer(answer)` - æ ‡å‡†åŒ–ç­”æ¡ˆç”¨äºæ¯”è¾ƒ  
- âœ… `compute_causal_lm_metrics(eval_preds)` - åŸºç¡€ LM æŒ‡æ ‡
- âœ… `compute_math_generation_metrics(tokenizer)` - æ•°å­¦ä»»åŠ¡ä¸“ç”¨æŒ‡æ ‡
- âœ… æ›´æ–°äº† `get_metrics_function(task_name, tokenizer)` - æ”¯æŒè‡ªåŠ¨é€‰æ‹©

#### æ”¯æŒçš„æŒ‡æ ‡
- **Token Accuracy**: Token çº§åˆ«çš„é¢„æµ‹å‡†ç¡®ç‡
- **Answer Accuracy**: æå–ç­”æ¡ˆåçš„ç²¾ç¡®åŒ¹é…ç‡ï¼ˆä¸“ä¸ºæ•°å­¦ä»»åŠ¡ï¼‰

### 2. **æ›´æ–°äº† `trainer/trainer_preparation.py`**

åœ¨ `train_causal_lm_task` å‡½æ•°ä¸­ï¼š
- âœ… è‡ªåŠ¨è°ƒç”¨ `get_metrics_function` è·å–åˆé€‚çš„ metrics
- âœ… ä¼ é€’ tokenizer å‚æ•°ç”¨äºæ–‡æœ¬è§£ç 
- âœ… æ·»åŠ  `preprocess_logits_for_metrics` ä¼˜åŒ–å†…å­˜
- âœ… æ¡ä»¶æ€§æ·»åŠ  metricsï¼ˆå¦‚æœå¯ç”¨ï¼‰

### 3. **æ›´æ–°äº†é…ç½®æ–‡ä»¶**

`configs/smol/135m_metamath.yaml`:
- âœ… è®¾ç½® `metric_for_best_model: "answer_accuracy"`
- âœ… è®¾ç½® `greater_is_better: true`

### 4. **åˆ›å»ºäº†æ–‡æ¡£**

- âœ… `docs/METRICS_GUIDE.md` - å®Œæ•´çš„ metrics ä½¿ç”¨æŒ‡å—

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### æ™ºèƒ½ç­”æ¡ˆæå–
æ”¯æŒå¤šç§æ ¼å¼ï¼š
```python
"#### 42"                    â†’ "42"
"The answer is 42"           â†’ "42"  
"Final answer: 256"          â†’ "256"
"Therefore x = -5.5"         â†’ "-5.5"
"We get 1/2"                 â†’ "0.5" (æ ‡å‡†åŒ–å)
```

### è‡ªåŠ¨ä»»åŠ¡æ£€æµ‹
æ ¹æ® `task_name` è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ metricsï¼š
```python
"metamath_qa"    â†’ compute_math_generation_metrics
"gsm8k"          â†’ compute_math_generation_metrics
"glue_sst2"      â†’ GLUE metrics
"causal_lm"      â†’ compute_causal_lm_metrics
"unknown"        â†’ None (åªä½¿ç”¨ loss)
```

### å†…å­˜ä¼˜åŒ–
```python
# åªä¿å­˜ argmax é¢„æµ‹ï¼Œä¸ä¿å­˜å®Œæ•´ logits
def preprocess_logits_for_metrics(logits, labels):
    return logits.argmax(dim=-1)
```

## ğŸ“Š è®­ç»ƒæ—¶çš„è¾“å‡ºç¤ºä¾‹

### è¯„ä¼°æŒ‡æ ‡
```
***** Evaluation results *****
  eval_loss                = 1.2345
  eval_token_accuracy      = 0.8567
  eval_answer_accuracy     = 0.7234
  eval_runtime             = 12.34
  eval_samples_per_second  = 162.3
  eval_steps_per_second    = 5.12
```

### è°ƒè¯•æ—¥å¿—
```
============================================================
Sample predictions (for debugging):

Example 1:
  Prediction: Let's solve step by step... #### 42
  Label: The solution is... #### 42
  Extracted pred answer: 42
  Extracted label answer: 42
  Match: True

Example 2:
  Prediction: Calculate: 2+2 = #### 4
  Label: Answer: #### 4
  Extracted pred answer: 4
  Extracted label answer: 4
  Match: True

Example 3:
  Prediction: The result is #### 100
  Label: Final answer #### 99
  Extracted pred answer: 100
  Extracted label answer: 99
  Match: False
============================================================
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨
```bash
# è®­ç»ƒ MetaMathQAï¼Œè‡ªåŠ¨å¯ç”¨ç­”æ¡ˆå‡†ç¡®ç‡è¯„ä¼°
python train.py --config configs/smol/135m_metamath.yaml
```

### é…ç½®é€‰é¡¹
```yaml
task_name: "metamath_qa"  # è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æ•°å­¦ metrics

training:
  # ä½¿ç”¨ç­”æ¡ˆå‡†ç¡®ç‡é€‰æ‹©æœ€ä½³æ¨¡å‹
  metric_for_best_model: "answer_accuracy"
  greater_is_better: true
  load_best_model_at_end: true
  
  # æˆ–ä½¿ç”¨ token å‡†ç¡®ç‡
  # metric_for_best_model: "token_accuracy"
  
  # æˆ–åªä½¿ç”¨ loss
  # metric_for_best_model: "loss"
  # greater_is_better: false
```

## ğŸ¨ æ”¯æŒçš„ä»»åŠ¡ç±»å‹

| ä»»åŠ¡ç±»å‹ | task_name ç¤ºä¾‹ | Metrics | è¯´æ˜ |
|---------|---------------|---------|------|
| **æ•°å­¦æ¨ç†** | `metamath_qa`, `gsm8k`, `math_qa` | token_accuracy, answer_accuracy | æå–å¹¶æ¯”è¾ƒç­”æ¡ˆ |
| **GLUE** | `glue_sst2`, `glue_cola` | accuracy, f1, matthews_correlation | åˆ†ç±»ä»»åŠ¡æ ‡å‡†æŒ‡æ ‡ |
| **é€šç”¨ LM** | `causal_lm`, `language_model` | token_accuracy | åŸºç¡€ç”ŸæˆæŒ‡æ ‡ |
| **ä»£ç ç”Ÿæˆ** | `code_feedback`, `code_gen` | token_accuracy, answer_accuracy | å½“å‰ä½¿ç”¨æ•°å­¦ metrics |
| **å…¶ä»–** | ä»»æ„æœªåŒ¹é…çš„åç§° | None | åªä½¿ç”¨ loss |

## ğŸš€ ä¸ç°æœ‰ä»£ç çš„é›†æˆ

### å®Œå…¨å…¼å®¹
- âœ… ä¸å½±å“ç°æœ‰çš„ GLUE ä»»åŠ¡è®­ç»ƒ
- âœ… ä¸å½±å“åˆ†ç±»ä»»åŠ¡
- âœ… å‘åå…¼å®¹ï¼ˆå¦‚æœä¸éœ€è¦ metricsï¼Œè‡ªåŠ¨è·³è¿‡ï¼‰

### è‡ªåŠ¨å¯ç”¨
å¯¹äº MetaMathQAã€GSM8K ç­‰ä»»åŠ¡ï¼š
1. æ£€æµ‹ `task_name` åŒ…å« "math"ã€"metamath" æˆ– "gsm8k"
2. è‡ªåŠ¨åˆ›å»º `compute_math_generation_metrics(tokenizer)`
3. æ·»åŠ åˆ° trainer å‚æ•°
4. åœ¨æ¯æ¬¡è¯„ä¼°æ—¶è®¡ç®—å¹¶è®°å½•

### å¯é€‰æ€§
- å¦‚æœ `get_metrics_function` è¿”å› `None`ï¼Œtrainer åªä½¿ç”¨ loss
- è¿™å¯¹æœªçŸ¥ä»»åŠ¡ç±»å‹æ˜¯å®‰å…¨çš„

## ğŸ“ˆ æ€§èƒ½è€ƒè™‘

### å†…å­˜ä½¿ç”¨
- âœ… ä½¿ç”¨ `argmax` é¢„å¤„ç†ï¼Œé¿å…ä¿å­˜å®Œæ•´ logits
- âœ… æ‰¹é‡è§£ç ï¼Œæé«˜æ•ˆç‡
- âœ… åªåœ¨éªŒè¯é›†ä¸Šè®¡ç®—ï¼Œä¸å½±å“è®­ç»ƒé€Ÿåº¦

### è®¡ç®—å¼€é”€
- Token accuracy: æå°ï¼ˆåªæ˜¯å¼ é‡æ¯”è¾ƒï¼‰
- Answer accuracy: ä¸­ç­‰ï¼ˆéœ€è¦æ–‡æœ¬è§£ç å’Œæ­£åˆ™æå–ï¼‰
- æ€»ä½“å½±å“: < 5% çš„è¯„ä¼°æ—¶é—´å¢åŠ 

### ä¼˜åŒ–å»ºè®®
```yaml
training:
  per_device_eval_batch_size: 16  # å¢å¤§ä»¥æé«˜è¯„ä¼°é€Ÿåº¦
  eval_steps: 500                  # å‡å°‘è¯„ä¼°é¢‘ç‡
```

## ğŸ” è°ƒè¯•å’ŒéªŒè¯

### æŸ¥çœ‹ metrics æ˜¯å¦å¯ç”¨
```
INFO - Training causal LM task
INFO - Using metrics for task: metamath_qa
```

### æŸ¥çœ‹å…·ä½“æŒ‡æ ‡
è¯„ä¼°æ—¥å¿—ä¼šæ˜¾ç¤ºï¼š
```
eval_token_accuracy: 0.8567
eval_answer_accuracy: 0.7234
```

### æŸ¥çœ‹æ ·ä¾‹é¢„æµ‹
æ¯æ¬¡è¯„ä¼°ä¼šè®°å½•å‰ 3 ä¸ªæ ·ä¾‹çš„è¯¦ç»†ä¿¡æ¯

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°å­¦ä»»åŠ¡
```yaml
task_name: "metamath_qa"
training:
  metric_for_best_model: "answer_accuracy"  # æœ€é‡è¦çš„æŒ‡æ ‡
  greater_is_better: true
```

### 2. é€šç”¨ç”Ÿæˆä»»åŠ¡
```yaml
task_name: "causal_lm"
training:
  metric_for_best_model: "token_accuracy"  # æˆ– "loss"
```

### 3. åˆ†ç±»ä»»åŠ¡ï¼ˆä¿æŒä¸å˜ï¼‰
```yaml
task_name: "glue_sst2"
training:
  metric_for_best_model: "accuracy"
  greater_is_better: true
```

## ğŸ“ ä»£ç ç¤ºä¾‹

### æ‰‹åŠ¨ä½¿ç”¨ metrics
```python
from utils.metrics import (
    extract_math_answer,
    normalize_answer,
    compute_math_generation_metrics
)
from transformers import AutoTokenizer

# æå–ç­”æ¡ˆ
text = "The calculation gives us #### 42"
answer = extract_math_answer(text)  # "42"

# æ ‡å‡†åŒ–
norm = normalize_answer("42.0")  # "42"

# åˆ›å»º metrics å‡½æ•°
tokenizer = AutoTokenizer.from_pretrained("model_name")
compute_fn = compute_math_generation_metrics(tokenizer)

# åœ¨è¯„ä¼°æ—¶ä½¿ç”¨
metrics = compute_fn((predictions, labels))
print(metrics)  # {"token_accuracy": 0.85, "answer_accuracy": 0.72}
```

## âœ… éªŒè¯æ¸…å•

- [x] `utils/metrics.py` - æ·»åŠ äº†æ•°å­¦ä»»åŠ¡ metrics
- [x] `trainer/trainer_preparation.py` - é›†æˆåˆ° trainer
- [x] `configs/smol/135m_metamath.yaml` - é…ç½®æ›´æ–°
- [x] `docs/METRICS_GUIDE.md` - å®Œæ•´æ–‡æ¡£
- [x] å‘åå…¼å®¹æ€§ - ä¸å½±å“ç°æœ‰åŠŸèƒ½
- [x] è‡ªåŠ¨æ£€æµ‹ - æ ¹æ® task_name é€‰æ‹©
- [x] å†…å­˜ä¼˜åŒ– - ä½¿ç”¨ argmax é¢„å¤„ç†
- [x] è°ƒè¯•å‹å¥½ - è®°å½•æ ·ä¾‹é¢„æµ‹

## ğŸ‰ æ€»ç»“

ç°åœ¨é¡¹ç›®å¯¹ MetaMathQA çš„æ”¯æŒåŒ…æ‹¬ï¼š

1. âœ… **å®Œæ•´çš„æ•°æ®å¤„ç†** - æ ¼å¼åŒ– + tokenization
2. âœ… **æ­£ç¡®çš„ data collator** - DataCollatorForLanguageModeling
3. âœ… **æ™ºèƒ½çš„è¯„ä¼°æŒ‡æ ‡** - token_accuracy + answer_accuracy
4. âœ… **è‡ªåŠ¨ä»»åŠ¡æ£€æµ‹** - æ ¹æ® task_name é€‰æ‹© metrics
5. âœ… **ä¼˜åŒ–çš„æ€§èƒ½** - å†…å­˜å‹å¥½çš„å®ç°

**ç«‹å³å¼€å§‹ä½¿ç”¨**:
```bash
python train.py --config configs/smol/135m_metamath.yaml
```

è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨ï¼š
- ğŸ“Š è®¡ç®— token å‡†ç¡®ç‡
- ğŸ¯ æå–å¹¶æ¯”è¾ƒç­”æ¡ˆ
- ğŸ’¾ ä¿å­˜æœ€ä½³ç­”æ¡ˆå‡†ç¡®ç‡çš„æ¨¡å‹
- ğŸ“ è®°å½•è¯¦ç»†çš„è¯„ä¼°æ—¥å¿—

**å®Œç¾æ”¯æŒ MetaMathQAï¼** ğŸš€
