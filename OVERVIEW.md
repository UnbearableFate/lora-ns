# PEFT Training Framework - Project Overview

## ğŸ“ Project Structure

```
lora-ns/
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ glue_mrpc.yaml           # GLUE benchmark (classification)
â”‚   â”œâ”€â”€ metamath_qa.yaml         # MetaMathQA (math reasoning)
â”‚   â”œâ”€â”€ gsm8k.yaml               # GSM8K (grade school math)
â”‚   â”œâ”€â”€ code_feedback.yaml       # Code generation tasks
â”‚   â”œâ”€â”€ template.yaml            # Template for custom tasks
â”‚   â”œâ”€â”€ accelerate_config.yaml   # Multi-GPU training config
â”‚   â””â”€â”€ deepspeed_config.json    # DeepSpeed ZeRO config
â”‚
â”œâ”€â”€ utils/                        # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_utils.py          # YAML config loading/validation
â”‚   â”œâ”€â”€ model_utils.py           # Model loading with PEFT
â”‚   â”œâ”€â”€ dataset_loader.py        # Dataset loading/preprocessing
â”‚   â””â”€â”€ metrics.py               # Evaluation metrics
â”‚
â”œâ”€â”€ examples/                     # Example scripts
â”‚   â”œâ”€â”€ quick_start.py           # Quick training example
â”‚   â”œâ”€â”€ train_glue.sh            # GLUE training script
â”‚   â”œâ”€â”€ train_metamath.sh        # MetaMathQA training script
â”‚   â”œâ”€â”€ train_multi_gpu.sh       # Multi-GPU training script
â”‚   â””â”€â”€ run_inference.sh         # Inference script
â”‚
â”œâ”€â”€ train.py                      # Main training script
â”œâ”€â”€ inference.py                  # Inference script
â”œâ”€â”€ merge_adapter.py              # Merge LoRA with base model
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # Documentation
â””â”€â”€ .gitignore                    # Git ignore patterns
```

## ğŸš€ Quick Start

### 1. Installation
```bash
pip install -r requirements.txt
```

### 2. Train a Model
```bash
# Classification task (GLUE MRPC)
python train.py --config configs/glue_mrpc.yaml

# Math reasoning (GSM8K)
python train.py --config configs/gsm8k.yaml

# Code generation
python train.py --config configs/code_feedback.yaml
```

### 3. Run Inference
```bash
python inference.py \
  --config configs/gsm8k.yaml \
  --model_path ./outputs/gsm8k \
  --input_text "What is 25 * 4?"
```

## ğŸ“Š Supported Tasks

| Task | Dataset | Model Example | PEFT Method | Training Time* |
|------|---------|---------------|-------------|----------------|
| **Text Classification** | GLUE | BERT, RoBERTa | LoRA | ~1 hour |
| **Math Reasoning** | MetaMathQA | Llama-2, Mistral | LoRA | ~10 hours |
| **Grade School Math** | GSM8K | Llama-2, Mistral | LoRA | ~5 hours |
| **Code Generation** | Code-Feedback | CodeLlama | LoRA | ~8 hours |

*Approximate times on a single A100 GPU

## ğŸ”§ Configuration System

All tasks are configured via YAML files with the following structure:

```yaml
task_name: "your_task"
task_type: "causal_lm"  # or "classification"

model:
  name_or_path: "model-name"
  
peft:
  method: "lora"
  lora_r: 16
  lora_alpha: 32
  
dataset:
  name: "dataset-name"
  prompt_template: |
    Your template here
    
training:
  num_train_epochs: 3
  learning_rate: 2e-4
  # ... more options
```

## ğŸ’¡ Key Features

### 1. PEFT Methods
- **LoRA**: Low-Rank Adaptation (most popular)
- **Prefix Tuning**: Add trainable prefix tokens
- **Prompt Tuning**: Soft prompt optimization

### 2. Quantization Support
- 4-bit quantization (QLoRA)
- 8-bit quantization
- BFloat16 training

### 3. Multi-GPU Training
```bash
# Using Accelerate
accelerate launch train.py --config configs/metamath_qa.yaml

# Using DeepSpeed
accelerate launch --config_file configs/accelerate_config.yaml \
  train.py --config configs/metamath_qa.yaml
```

### 4. Model Merging
```bash
python merge_adapter.py \
  --config configs/gsm8k.yaml \
  --adapter_path ./outputs/gsm8k \
  --output_path ./merged_model
```

## ğŸ“ Creating Custom Tasks

1. Copy template configuration:
```bash
cp configs/template.yaml configs/my_task.yaml
```

2. Edit configuration:
   - Set model name
   - Configure dataset
   - Adjust prompt template
   - Tune hyperparameters

3. Train:
```bash
python train.py --config configs/my_task.yaml
```

## ğŸ¯ Best Practices

### Memory Optimization
- Use gradient checkpointing
- Enable quantization (4-bit/8-bit)
- Reduce batch size, increase gradient accumulation
- Use bf16 instead of fp16

### LoRA Configuration
- Small datasets: r=8, alpha=16
- Large datasets: r=16-32, alpha=32-64
- Target all linear layers for best performance

### Learning Rate
- Classification: 3e-4 to 5e-4
- Causal LM: 1e-4 to 2e-4
- Use cosine scheduler with warmup

## ğŸ“š Additional Resources

- HuggingFace Transformers: https://huggingface.co/docs/transformers
- PEFT Library: https://huggingface.co/docs/peft
- TRL (Transformer Reinforcement Learning): https://huggingface.co/docs/trl
- Accelerate: https://huggingface.co/docs/accelerate

## ğŸ› Troubleshooting

### CUDA Out of Memory
```yaml
# Reduce batch size
per_device_train_batch_size: 2
gradient_accumulation_steps: 16

# Enable optimizations
gradient_checkpointing: true
optim: "paged_adamw_8bit"
```

### Slow Training
```yaml
# Use mixed precision
bf16: true

# Optimize batch size
per_device_train_batch_size: 8  # Increase if memory allows
```

### Poor Results
- Increase LoRA rank (r)
- Train for more epochs
- Check data preprocessing
- Validate prompt template

## ğŸ“§ Support

For issues and questions:
1. Check the README.md
2. Review example scripts in `examples/`
3. Examine configuration templates
4. Consult HuggingFace documentation

## ğŸ™ Acknowledgments

Built with:
- HuggingFace Transformers & PEFT
- PyTorch
- Accelerate & DeepSpeed
- Dataset providers (GLUE, MetaMathQA, GSM8K, Code-Feedback)
